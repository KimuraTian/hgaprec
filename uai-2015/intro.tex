\section{INTRODUCTION}
Recommendation systems are a vital component of the modern Web.  They
help readers effectively navigate otherwise unwieldy archives of
information and help websites direct users to items---movies,
articles, songs, products---that they will like.  A recommendation
system is built from historical data about which items each user has
consumed, be it clicked, viewed, rated, or purchased. First, it
uncovers the behavioral patterns that characterize various types of
users and the kinds of items they tend to like.  Then, it exploits
these discovered patterns to recommend future items to its users.

In this paper, we develop Hierarchical Poisson factorization (HPF) for
generating high-quality recommendations. Our algorithms easily scale
to massive data and significantly outperform the existing methods. We
show HPF is tailored to real-world properties of user behavior data:
the heterogeneous interests of users, the varied types of items, and a
realistic distribution of the finite resources that users have to
consume these items.

%% \begin{figure*}[th]
%% \centering
%% \vspace{0.1cm}
%% \small
%% \begin{tabular}{c}
%% \bf{``Action''}\\
%% \midrule
%% The Matrix\\
%% The Matrix: Reloaded\\
%% Spider-Man\\
%% X2: X-Men United\\
%% \bottomrule
%% \end{tabular}
%% \begin{tabular}{c}
%% \bf{``Indie Comedy, Romance''}\\
%% \midrule
%% Grosse Pointe Blank\\
%% Four Weddings and a Funeral\\
%% High Fidelity\\
%% Much Ado About Nothing\\
%% \bottomrule
%% \end{tabular}
%% \begin{tabular}{c}
%% \bf{``80's Science Fiction''}\\
%% \midrule
%% Star Wars: Episode IV: A New Hope\\
%% Star Wars: Episode VI: Return of the Jedi\\
%% Star Wars: Episode V: The Empire Strikes Back\\
%% Back to the Future Part II\\
%% \bottomrule
%% \end{tabular}

%% \vspace{0.5cm}

%% \centering
%% \includegraphics[width=0.8\textwidth]{figures/netflix-exploratory.pdf}
%% \caption{The top panel shows the top movies in 3 components for a
%%   user from the Netflix data set. The bottom panel is an illustration
%%   showing a subset of the highly rated movies by this user, and the
%%   right panel shows movies recommended to the user by our
%%   algorithm. The expected user's $K$-vector of weights $\theta_u$,
%%   inferred by our algorithm is shown in the middle panel.}
%% \label{fig:netflix-illustration}
%% \end{figure*}

%% \myfig{netflix-illustration} illustrates Poisson factorization on data
%% from Netflix.  The Netflix data contains the ratings of 480,000 users
%% on 17,000 movies, organized in a sparse matrix of 8.16B cells (and
%% containing 250M ratings).  From these data, we extract the patterns of
%% users' interests and the movies that are associated with those
%% interests.  The left panel illustrates some of those patterns---the
%% algorithm has uncovered action movies, independent comedies, and 1980s
%% science fiction.  The top panel illustrates how we can use these
%% patterns to form recommendations for an (imaginary) user.  This user
%% enjoys various types of movies, including fantasy (``Lord of the
%% Rings''), classic science fiction (``Star Wars: Episode V''), and
%% independent comedies (``Clerks'', ``High Fidelity'').  Of course, she
%% has only seen a handful of the available movies.  PF first uses the
%% movies she has seen to infer what kinds of movies she is interested
%% in, and then uses these inferred interests to suggest new movies.  The
%% list of movies at the bottom of the figure was suggested by our
%% algorithm. It includes other comedies (such as ``The Big Lebowski'')
%% and other science fiction (such as ``Star Wars: Episode II'').

In more detail, HPF is a probabilistic model of users and items.  It
associates each user with a latent vector of preferences, each item
with a latent vector of attributes, and constrains both sets of
vectors to be sparse and non-negative. The model assumes that each
cell of the observed behavior matrix is drawn from a Poisson
distribution---an exponential family distribution over non-negative
integers---whose parameter is a linear combination of the
corresponding user preferences and item attributes.  The main
computational problem is posterior inference: given an observed matrix
of user behavior, we would like to discover the latent attributes that
describe the items and the latent preferences of the users, which we
can then use to make predictions and recommendations.

%% recys-prem !!! removed paragraph below (references fig 1)

%%  For example, the
%% components in \myfig{netflix-illustration} (left) illustrate the top
%% items for specific attribute dimensions and the plot in
%% \myfig{netflix-illustration} (middle) illustrates the estimated
%% preference vector for the given user.  A spike in the preference
%% vector implies that the user tends to like items with the
%% corresponding latent attribute.

This inferential computation is common to many variants of matrix
factorization.  We find, however, that HPF enjoys significant
quantitative advantages over classical methods for a variety of
\emph{implicit feedback} data sets. \myfig{precision_recall} shows
that HPF performs significantly better than competing
methods---including the industry standard of matrix factorization with
user and item biases (MF)---for large data sets of Netflix users
watching movies, Last.FM users listening to music, scientists reading
papers, and \textit{New York Times} readers clicking on articles.

We review related work in detail in \mysec{related}. We now discuss
details of the Poisson factorization model, including its statistical
properties and methods for scalable inference.

%%
%%\begin{figure}
%%\centering
%%\includegraphics[width=0.8\columnwidth]{figures/movielens-user.pdf}\\
%%\includegraphics[width=0.8\columnwidth]{figures/movielens-item.pdf}\\
%%\caption{The weights of the randomly chosen user $U$ (Top) in the
 %% movielens data set and the weights of her top recommended movie
  %%\emph{Shakespeare in Love} (Bottom) are shown. User $U$ views a
  %%variety of movies, and her weights span a range of factor. User $U$
  %%had 184 views in the data set of movies ranging from Drama, Comedy,
  %%Thriller to Musical. Of these movies, 126 were either 4 or 5
  %%stars. Movies are generally characterized by a sparse set of
  %%factors.}
%%\end{figure}



%% \begin{figure*}[t!]
%% \includegraphics[width=\textwidth]{figures/user_activity_cdf.pdf}
%% \caption{Empirical complimentary cumulative distributions of user
%%   activity on each data set. Each curve shows the fraction of users
%%   who have consumed at least a given number of items. For instance,
%%   slightly less than half of all Netflix users have rated at least
%%   100 movies.}
%% \label{fig:marginals}
%% \end{figure*}


%% \begin{figure*}[t!]
%%   \includegraphics[width=\textwidth]{figures/user_activity.pdf}
%%   \caption{Empirical distributions of user activity on each
%%     dataset. Each plot shows the number of users who have rated a given
%%     number of items. For instance, slightly less than half of all
%%     Netflix users have rated at least 100 movies.}
%% \label{fig:marginals}
%% \end{figure*}

%% \begin{figure*}[t!]
%% \includegraphics[width=0.33\textwidth]{figures/marginals/echonest.pdf}
%% \includegraphics[width=0.33\textwidth]{figures/marginals/nyt.pdf}
%% \includegraphics[width=0.33\textwidth]{figures/marginals/netflix.pdf}
%% \caption{Empirical distribution of item popularity on real datasets,
%%   with fitted negative binomial and Gaussian distributions. The
%%   distributions were fit using maximum likelihood estimation. The
%%   negative binomial places significant probability mass on the left
%%   tail, i.e., items with few ratings. The colored bars show that such
%%   items are the most frequent. In contrast, the Gaussian distribution
%%   places negligible mass on the left tail and mainly captures popular
%%   items. The mode of the negative binomial distribution is also closer
%%   to the empirical mode than the Gaussian distribution.}
%% \label{fig:marginals}
%% \end{figure*}

%% Further speed-ups using stochastic variational
%% inference~\cite{Hoffman:2013} let us fit Poisson factorization models
%% to massive data.

