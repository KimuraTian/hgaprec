\section{Appendix A: The variational algorithm}
Given an observed matrix of user behavior $y$, we would like to
compute the posterior distribution of user preferences $\theta_{uk}$,
item attributes $\beta_{ik}$, user activity $\xi_u$ and item
popularity $\eta_i$, $p(\theta, \beta, \xi, \eta \g y)$.  Our
derivation of the variational algorithm for HPF makes use of
general results about the class of \textit{conditionally conjugate}
models~\cite{Ghahramani:2001,Hoffman:2013}.  We define the class, show
that HPF is in the class, and then derive the variational
inference algorithm.

{\bf Complete conditionals.}  Variational inference fits the
variational parameters to minimize their KL divergence to the
posterior. For the large class of conditionally conjugate models, we
can easily perform this optimization with a coordinate-ascent
algorithm, one in which we iteratively optimize each variational
parameter while holding the others fixed.  A \textit{complete
conditional} is the conditional distribution of a latent variable
given the observations and the other latent variables in the model.  A
conditionally conjugate model is one where each complete conditional
is in an exponential family.

HPF, with the $z_{ui}$ variables described in \mysec{inference}, is a
conditionally conjugate model.  (Without the auxiliary variables, it
is not conditionally conjugate.) For the user weights $\theta_{uk}$,
the complete conditional is a Gamma,
\begin{equation}
  \label{eq:user-weight-cc}
  \theta_{uk} \g \beta, \xi, z, y \sim
  \gam(a + \textstyle \sum_{i} z_{uik}, \xi_u + \sum_{i} \beta_{ik}).
\end{equation}
The complete conditional for item weights $\beta_{ik}$ is symmetric,
\begin{equation}
  \label{eq:item-weight-cc}
  \beta_{ik} \g \theta, \eta, z, y \sim
  \gam(a + \textstyle \sum_{u} z_{uik}, \eta_i + \sum_{i} \theta_{uk}).
\end{equation}
These distributions stem from conjugacy properties between the Gamma
and Poisson. In the user weight distribution, for example, the item
weights $\beta_{ik}$ act as ``exposure'' variables~\cite{Gelman:1995}.
(The roles are reversed in the item weight distribution.) We can
similarly write down the complete conditionals for the user activity
$\xi_u$ and the item popularity $\eta_i$.
\begin{align*}
  \label{eq:user-weight-cc}
  \xi_{u} \g \theta \sim
  \gam(a' + \textstyle Ka, b' + \sum_{k} \theta_{uk}).\nonumber\\
  \eta_{i} \g \beta \sim
  \gam(c' + \textstyle Kc, d' + \sum_{k} \beta_{ik}).\nonumber\\
\end{align*}
The final latent variables are the auxiliary variables.  Recall that
each $z_{ui}$ is a $K$-vector of Poisson counts that sum to the
observation $y_{ui}$. The complete conditional for this vector is
\begin{equation}
  \label{eq:aux-cc}
  z_{ui} \g \beta, \theta, y \sim \mult\left(y_{ui}, \frac{\theta_{u} 
      \beta_{i}}{\textstyle \sum_{k} \theta_{uk} \beta_{ik}}\right).
\end{equation}
Though these variables are Poisson in the model, their complete
conditional is multinomial.  The reason is that the conditional
distribution of a set of Poisson variables, given their sum, is a
multinomial for which the parameter is their normalized set of
rates. (See ~\cite{Johnson:2005, Cemgil:2009}.)

{\bf Deriving the algorithm.}
We now derive variational inference for HPF. First, we set each
factor in the mean-field family (\myeq{q}) to be the same type of
distribution as its complete conditional.  The complete conditionals
for the item weights $\beta_{ik}$ and user weights $\theta_{uk}$ are
Gamma distributions (Equations \ref{eq:user-weight-cc} and
\ref{eq:item-weight-cc}); thus the variational parameters
$\lambda_{ik}$ and $\gamma_{uk}$ are Gamma parameters, each containing
a shape and a rate.  Similarly, the variational user activity
parameters $\kappa_u$ and the variational item popularity parameter
$\tau_i$ are Gamma parameters, each containing a shape and a rate.
The complete conditional of the auxiliary variables $z_{uik}$ is a
multinomial (\myeq{aux-cc}); thus the variational parameter
$\phi_{ui}$ is a multinomial parameter, a point on the $K$-simplex,
and the variational distribution for $z_{ui}$ is $\mult(y_{ui},
\phi_{ui})$.

In coordinate ascent we iteratively optimize each variational
parameter while holding the others fixed.  In conditionally conjugate
models, this amounts to setting each variational parameter equal to
the expected parameter (under $q$) of the complete conditional.
\footnote{It is a little more complex then this. For details, see~\cite{Hoffman:2013}.}  
The parameter to each complete conditional is a function of the other
latent variables and the mean-field family sets all the variables to
be independent.  These facts guarantee that the parameter we are
optimizing will not appear in the expected parameter.

For the user and item weights, we update the variational shape and
rate parameters. The updates are
\begin{eqnarray}
  \gamma_{uk} &=& \langle a + \textstyle \sum_{i} y_{ui} \phi_{uik},
  b + \textstyle \sum_i \lambda_{ik}^{\shape} / \lambda_{ik}^{\rate} \rangle \\
  \lambda_{ik} &=& \langle c + \textstyle \sum_{u} y_{ui} \phi_{uik},
  d + \textstyle \sum_u \gamma_{ik}^{\shape} / \gamma_{ik}^{\rate} \rangle.
\end{eqnarray}
These are expectations of the complete conditionals in
Equations~\ref{eq:user-weight-cc} and \ref{eq:item-weight-cc}.  In the
shape parameter, we use that the expected count of the $k$th item in
the multinomial is $\E_q[z_{uik}] = y_{ui} \phi_{uik}$. In the rate
parameter, we use that the expectation of a Gamma variable is the
shape divided by the rate.

For the variational multinomial the update is
\begin{equation}
  \phi_{ui} \propto \exp\{\Psi(\gamma_{uk}^\shape) - \log
  \gamma_{uk}^{\rate} + \Psi(\lambda_{ik}^\shape) - \log
  \lambda_{ik}^\rate\},
\end{equation}
where $\Psi(\cdot)$ is the digamma function (the first derivative of
the log $\Gamma$ function).  This update comes from the expectation of
the log of a Gamma variable, for example $\E_q[\log \theta_{uk}] =
\Psi(\gamma_{nk}^\shape) - \log \gamma_{nk}^{\rate}$.

