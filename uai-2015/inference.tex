\subsection{Inference with variational methods}
\label{sec:inference}

Using HPF for recommendation hinges on solving the posterior inference
problem.  Given a set of observed ratings, we would like to infer the
user preferences and item attributes that explain these ratings, and
then use these inferences to recommend new content to the users.  In
this section we discuss the details and practical challenges of
posterior inference for HPF, and present a mean-field variational
inference algorithm as a practical and scalable approach.  Our
algorithm easily accommodates data sets with millions of users and
hundreds of thousands of items on a single CPU.

Given a matrix of user behavior, we would like to
compute the posterior distribution of user preferences $\theta_{uk}$,
item attributes $\beta_{ik}$, user activity $\xi_{u}$ and item
popularity $\eta_i$.  As for many Bayesian models, however, the exact
posterior is computationally intractable. We show how to
efficiently approximate the posterior with mean-field variational
inference.

% \begin{equation*}
%   p(\theta, \beta, \xi, \eta \g y) = \frac{p(\theta | \xi) p(\xi) 
%     p(\eta) p(\beta | \eta) p(y \g \theta, \beta, \xi, \eta)}
%   {\int_{\theta} \int_{\xi} \int_{\eta} \int_{\beta} p(\theta |
%     \xi) p(\xi) p(\beta) p(\beta | \eta) p(y \g \theta, \beta, \xi, \eta)}. 
% \end{equation*}


Variational inference is an optimization-based strategy for
approximating posterior distributions in complex probabilistic
models~\cite{Jordan:1999,Wainwright:2008}.  Variational algorithms
posit a family of distributions over the hidden variables, indexed by
free ``variational'' parameters, and then find the member of that
family that is closest in Kullback-Liebler (KL) divergence to the true
posterior.  (The form of the family is chosen to make this
optimization possible.)  Thus, variational inference turns the
inference problem into an optimization problem.  Variational inference
has previously been used for large-scale
recommendation~\cite{Paquet:2013p9197}.

%% tends to scale better than alternative sampling-based approaches, like
%% Monte Carlo Markov chain sampling, and has been deployed to solve many
%% applied problems with complex models, including 

% prem: remove stochastic
%% Finally, we develop a scalable stochastic variational inference
%% algorithm for massive data.  This algorithm repeatedly subsamples
%% users from the matrix and updates its estimates of the latent
%% representations.

We will describe a simple variational inference algorithm for HPF.  To
do so, however, we first give an alternative formulation of the model
in which we add an additional layer of latent variables.  These
auxiliary variables facilitate derivation and description of the
algorithm~\cite{Ghahramani:2001,Hoffman:2013}.

For each user and item we add $K$ latent variables $z_{uik} \sim
\poisson(\theta_{uk} \beta_{ik})$, which are integers that sum to the
user/item value $y_{ui}$.  A sum of Poisson random variables is itself
a Poisson with rate equal to the sum of the rates.  Thus, these new
latent variables preserve the marginal distribution of the
observation, $y_{ui} \sim \poisson(\theta_{u}^\top \beta_{i})$.  These
variables can be thought of as the contribution from component $k$ to
the total observation $y_{ui}$.  Note that when $y_{ui} = 0$, these
auxiliary variables are not random---the posterior distribution of
$z_{ui}$ will place all its mass on the zero vector.  Consequently,
our inference procedure need only consider $z_{ui}$ for those
user/item pairs where $y_{ui} > 0$.


\begin{figure}[th]
  \begin{framed}
    For all users and items, initialize the user parameters
    $\gamma_u$, $\kappa_u^{\rate}$ and item parameters $\lambda_i$,
    $\tau_i^{\rate}$ to the prior with a small random offset. Set the
    user activity and item popularity shape parameters:
    \begin{align}
      \kappa_u^{\shape} = a' + Ka; \quad \tau_i^{\shape} = c' + Kc\nonumber
    \end{align}

    %\vspace{0.1in}

    Repeat until convergence:
    \begin{enumerate}
    \item For each user/item such that $y_{ui} > 0$, update the multinomial:
      \begin{equation*}
        \phi_{ui} \propto \exp\{\Psi(\gamma_{uk}^\shape) - \log
        \gamma_{uk}^{\rate} + \Psi(\lambda_{ik}^\shape) - \log
        \lambda_{ik}^\rate\}.
      \end{equation*}
    \item For each user, update the user weight and activity parameters:
      \begin{align}
        \gamma_{uk}^\shape & = a + \textstyle \sum_{i} y_{ui}
        \phi_{uik} \nonumber\\
        \gamma_{uk}^\rate & = \frac{\kappa_u^{\shape}}{\kappa_u^{\rate}} + \textstyle \sum_i \lambda_{ik}^{\shape} / \lambda_{ik}^{\rate}\nonumber\\
        \kappa_{u}^\rate & = \frac{a'}{b'} + \sum_k \frac{\gamma_{uk}^{\shape}}{\gamma_{uk}^{\rate}}\nonumber
      \end{align}
    \item For each item, update the item weight and popularity parameters:
      \begin{align}
        \lambda_{ik}^\shape & = c + \textstyle \sum_{u} y_{ui}
        \phi_{uik}\nonumber\\
        \lambda_{ik}^\rate & = \frac{\tau_i^{\shape}}{\tau_i^{\rate}} + \textstyle \sum_u
        \gamma_{uk}^{\shape} / \gamma_{uk}^{\rate}\nonumber\\
        \tau_{i}^\rate & = \frac{c'}{d'} + \sum_k \frac{\lambda_{ik}^{\shape}}{\lambda_{ik}^{\rate}}\nonumber
      \end{align}
    \end{enumerate}
    \vspace{-0.2in}
\end{framed}
    \vspace{-0.2in}
\caption{\label{fig:batch}Variational inference for Poisson
  factorization.  Each iteration only needs to consider the non-zero
  elements of the user/item matrix.}
\end{figure}

With these latent variables in place, we now describe the algorithm.
First, we posit the variational family over the hidden variables.
Then we show how to optimize its parameters to find the member close
to the posterior of interest.

The latent variables in the model are user weights $\theta_{uk}$, item
weights $\beta_{ik}$, and user-item contributions $z_{uik}$, which we
represent as a $K$-vector of counts $z_{ui}$.  The \textit{mean-field
  family} considers these variables to be independent and each
governed by its own distribution,
\begin{align}
  \label{eq:q}
  q(\beta, \theta, \xi, \eta, z) & =  \prod_{i,k} q(\beta_{ik} \g \lambda_{ik})
  \prod_{u,k} q(\theta_{uk} \g \gamma_{uk}) \nonumber\\
  & \prod_{u} q(\xi_u \g \kappa_u) \prod_{i} q(\eta_i \g \tau_i)
  \prod_{u,i} q(z_{ui} \g \phi_{ui}).
\end{align}
Though the variables are independent, this is a flexible family of
distributions because each variable is governed by its own free
parameter.  The variational factors for preferences $\theta_{uk}$,
attributes $\beta_{ik}$, activity $\xi_u$, and popularity $\eta_i$ are
all Gamma distributions, with freely set scale and rate variational
parameters. The variational factor
for $z_{ui}$ is a free multinomial, i.e., $\phi_{ui}$ is a $K$-vector
that sums to one.  This form stems from $z_{ui}$ being a bank of
Poisson variables conditional on a fixed sum $y_{ui}$, and the
property that such conditional Poissons are distributed as a
multinomial~\cite{Johnson:2005, Cemgil:2009}.

% \begin{equation*}
%   \nu^* = \arg \min_\nu \textrm{KL}(q(\beta,
%   \theta, \kappa, \tau, z \g \nu) || p(\beta, \theta, \kappa, \tau, z \g y)). 
% \end{equation*}


After specifying the family, we fit the variational parameters $\nu =
\{\lambda, \gamma, \kappa, \tau, \phi\}$ to minimize the KL divergence
to the posterior, and then use the corresponding variational
distribution $q(\cdot \g \nu^*)$ as its proxy. The mean-field
factorization facilitates both optimizing the variational objective
and downstream computations with the approximate posterior, such as
the recommendation score of \myeq{score}.

% dmb: !!! myeq{score} seems undefined.

We optimize the variational parameters with a coordinate ascent
algorithm, iteratively optimizing each parameter while holding the
others fixed.  The algorithm is illustrated in \myfig{batch}. We
denote shape with the superscript ``shp'' and rate with the
superscript ``rte''. We provide a detailed derivation in the Appendix.

Note that our algorithm is very efficient on sparse matrices. In step
1, we need only update variational multinomials for the non-zero
user/item observations $y_{ui}$.  In steps 2 and 3, the sums over
users and items need only to consider non-zero observations.  This
efficiency is thanks the likelihood of the full matrix only depending
on the non-zero observations, as we discussed in the previous section.
Both HPF and BPF enjoy this property and have the same computational
overhead, but HPF allows for more flexibility in modeling the variation
in activity and popularity across users and items, respectively.

% We note that fitting classical matrix factorization with squared loss
% must iteratively consider every cell of the matrix, both zeros and
% non-zeros. This makes MF difficult to fit with large matrices, though
% there are innovative solutions based on stochastic
% optimization~\cite{Mairal:2010}.

% dmb: above, i took this out.  we spent the whole last section
% trashing MF.  i don't think we need to repeat this point.


%% Implemented in parallel, the coordinate updates are equal to the
%% natural gradient of the variational objective. We found that following
%% these natural gradients~\cite{Hoffman:2013, Honkela:2008,Sato:2012}
%% (with step size one) resulted in a better way to fit the objective.
%% The algorithm in \myfig{batch} remains the same, except we now compute
%% steps 2 and 3 in parallel, updating the variational shape and rate
%% parameters of all users and items using the multinomial $\phi_{ui}$
%% parameters.

We terminate the algorithm when the variational distribution
converges. Convergence is measured by computing the prediction
accuracy on a validation set.  Specifically, we approximate the
probability that a user consumed an item using the variational
approximations to posterior expectations of $\theta_u$ and $\beta_i$,
and compute the average predictive log likelihood of the validation
ratings. The HPF algorithm stops when the change in log likelihood is
less than 0.0001\%. We find that the algorithm
is largely insensitive to small changes in the hyper-parameters. To
enforce sparsity, we set the shape hyperparameters $a'$, $a$, $c$ and
$c'$ to provide exponentially shaped prior \gam~distributions. We
fixed each hyperparameter at $0.3$. We set the hyperparameters $b'$
and $d'$ to 1, fixing the prior mean at 1.

% !!! DMB: clean up and move this to an appendix
