\section{Discussion}
We have demonstrated that Poisson factorization is an efficient and
effective means of generating high quality recommendations across a
variety of data sets ranging from movie views to scientific article
libraries. It significantly outperforms a number of leading
recommendation methods in modeling implicit behavior data without the
need for ad hoc modifications. Variational inference for HPF scales to
massive data and differs from traditional methods in its ability to
capture the heterogeneity amongst users and items, accounting for the
wide range of activity and popularity amongst them, respectively. The
HPF algorithm is a robust, off-the-shelf tool, providing high accuracy
even with fixed hyperparameter settings.

%%% recsys-prem !!! added note about SVI here (removed from intro)

Finally, we emphasize that HPF is more than just one method---it is
the simplest in a class of probabilistic models with these properties,
and is easily modified to include more complex structure and
assumptions~\cite{gopalan2014content,gopalan2014bayesian}.  Future
work includes bringing the confidence-weighting of~\cite{Hu:2008p9402}
into HPF to provide greater control and modeling flexibility in the
downweighting of zeros.

%% exploring modifications to BPF to incorporate
%% additional features---e.g., user and item metadata---as well as
%% stochastic inference to scale to massive data sets and Bayesian
%% non-parametric extensions~\cite{Zhou:2012}.


%% In settings where there are many more items than the typical user
%% can consume, unobserved consumption is likely explained by finite
%% attention, as opposed to an active dislike for the associated
%% content. Likewise, a user's choice in selecting a particular set of
%% items amongst the many available options is a relatively strong
%% indicator of her interests. BPF captures these features of sparse user
%% data via the Poisson likelihood, which appropriately balances strong
%% signals of consumption with weaker signals of unobserved activity.

%% Conveniently, the same Poisson likelihood also leads to
%% computationally efficient inference on sparse data sets, as it requires
%% evaluation of only the consumed user-item pairs, which comprise a
%% small fraction of all possible observations. This avoids the issue
%% faced by traditional matrix factorization in down-weighting or sampling
%% negative examples during training. In addition to this computational
%% advantage, BPF empirically outperforms classical MF across a wide array
%% of data sets---from movies to music to scientific articles---in
%% recommending relevant content to users.

%Future work includes exploring modifications to BPF to incorporate
%additional features---e.g., user and item metadata---as well as
%stochastic inference to scale to massive data sets.



%% There has also been significant research on Bayesian nonparametric
%% Poisson factor models. Using a Beta-Negative-Binomial process prior
%% for Poisson factor analysis the authors of ~\cite{Zhou:2012}
%% demonstrate that NMF, LDA and GaP are special cases of a finite
%% approximation to their model. Inference for the models
%% in~\cite{Zhou:2012} do not scale to the size of datasets we consider
%% here.
