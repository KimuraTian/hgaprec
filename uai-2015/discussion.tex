\section{DISCUSSION}
\vspace{-0.1in}
We have demonstrated that Poisson factorization is an efficient and
effective means of generating high quality recommendations across a
variety of data sets ranging from movie views to scientific article
libraries. It significantly outperforms a number of leading methods in
modeling implicit behavior data without the need for ad hoc
modifications. Variational inference for HPF scales to massive data
and differs from traditional methods in its ability to capture the
heterogeneity amongst users and items, accounting for the wide range
of activity and popularity amongst them, respectively. The HPF
algorithm is a robust, off-the-shelf tool, providing high accuracy
even with fixed hyperparameter settings.

%%% recsys-prem !!! added note about SVI here (removed from intro)
Finally, we emphasize that HPF is more than just one method---it is
the simplest in a class of probabilistic models with these properties,
and has already been extended to a combined model of article content
and reader ratings~\cite{gopalan2014content}, and a Bayesian
nonparametric model that adapts the dimensionality of the latent
representations~\cite{gopalan2014bayesian}.

A notable innovation in Gaussian MF is the algorithm
of~\cite{Hu:2008p9402} that explicitly downweights zeros using
confidence parameters. We presented the empirical study in this paper
comparing to the Gaussian MF with subsampled zeros~\cite{Koren:2009}.
We attempted to compare to a GraphChi
implementation~\cite{kyrola2012graphchi} of~\cite{Hu:2008p9402}, but
it gave unexpectedly poor results suggesting an incorrect
implementation. We found a correct implementation,
but these comparisons are ongoing work. Another piece of ongoing work
includes bringing the confidence-weighting of~\cite{Hu:2008p9402} into
HPF. This will allow downweighting of the zeros beyond that provided
implicitly by Poisson factorization.

%% we present our empirical study
%% without hu et al., but discuss the need to downweight the zeros on
%% even heavier tailed data sets.  we can note that PF dominates gaussian
%% factorization everywhere, but one innovation in gaussian factorization
%% is hu et al. (even if it is unprincipled).  we can talk about ``pilot
%% studies'' against hu et al. which found better performance in most
%% cases, and always for the more popular items.  an area of future work
%% to bring its intuition into PF.

%% This will allow us to provide a fair comparison to the Gaussian
%% MF with downweighted zeros~\cite{Hu:2008p9402}. 

%% , we are comparing to the Gaussian
%% MF with downweighted zeros~\cite{Hu:2008p9402}.

%%  included the 

%% Future work 

%% exploring modifications to BPF to incorporate
%% additional features---e.g., user and item metadata---as well as
%% stochastic inference to scale to massive data sets and Bayesian
%% non-parametric extensions~\cite{Zhou:2012}.


%% In settings where there are many more items than the typical user
%% can consume, unobserved consumption is likely explained by finite
%% attention, as opposed to an active dislike for the associated
%% content. Likewise, a user's choice in selecting a particular set of
%% items amongst the many available options is a relatively strong
%% indicator of her interests. BPF captures these features of sparse user
%% data via the Poisson likelihood, which appropriately balances strong
%% signals of consumption with weaker signals of unobserved activity.

%% Conveniently, the same Poisson likelihood also leads to
%% computationally efficient inference on sparse data sets, as it requires
%% evaluation of only the consumed user-item pairs, which comprise a
%% small fraction of all possible observations. This avoids the issue
%% faced by traditional matrix factorization in down-weighting or sampling
%% negative examples during training. In addition to this computational
%% advantage, BPF empirically outperforms classical MF across a wide array
%% of data sets---from movies to music to scientific articles---in
%% recommending relevant content to users.

%Future work includes exploring modifications to BPF to incorporate
%additional features---e.g., user and item metadata---as well as
%stochastic inference to scale to massive data sets.



%% There has also been significant research on Bayesian nonparametric
%% Poisson factor models. Using a Beta-Negative-Binomial process prior
%% for Poisson factor analysis the authors of ~\cite{Zhou:2012}
%% demonstrate that NMF, LDA and GaP are special cases of a finite
%% approximation to their model. Inference for the models
%% in~\cite{Zhou:2012} do not scale to the size of datasets we consider
%% here.
