\begin{figure*}[t!]
\centering
\includegraphics[width=0.8\textwidth]{newfigs/mean_precision_at_20.pdf}\\
\includegraphics[width=0.8\textwidth]{newfigs/mean_recall_at_20.pdf}\\
\caption{Predictive performance on data sets. The top and bottom plots
  show normalized mean precision and mean recall at 20
  recommendations, respectively. While competing method performance
  varies across data sets, HPF consistently outperforms competing
  methods.}
\label{fig:precision_recall}
\end{figure*}


\section{Empirical Study}
\label{sec:eval}
We evaluate the performance of the Hierarchical Poisson factorization
(HPF) algorithm on a variety of
large-scale user behavior data sets: users listening to music, users
watching movies, users reading scientific articles, and users reading
the newspaper.  We find that HPF provides significantly better
recommendations than competing methods.

We first discuss the details of each data set and of the competing
recommendation methods. We then describe our study, noting the
superior performance and computational efficiency of HPF.  We conclude
with an exploratory analysis of preferences and attributes on several
of the data sets.

{\bf Data Sets.} We study the HPF algorithm in Figure~\ref{fig:batch}
on several data sets of user behavior:
\begin{itemize}
\item The {\bf Mendeley} data set~\cite{Jack:2010} of scientific
  articles is a binary matrix of 80,000 users and 260,000 articles,
  with 5 million observations.  Each cell corresponds to the presence
  or absence of an article in a scientist's online library.
\item The {\bf Echo Nest} music data set~\cite{Bertin-Mahieux:2011} is
  a matrix of 1 million users and 385,000 songs, with 48 million
  observations.  Each observation is the number of times a user played
  a song.
\item The {\bf New York Times} data set is a matrix of 1,615,675 users
  and 103,390 articles, with 80 million observations.  Each
  observation is the number of times a user viewed an article.

\item The {\bf Netflix} data set~\cite{Koren:2009} contains 480,000
  users and 17,770 movies, with 100 million observations. Each
  observation is the rating (from 1 to 5 stars) that a user provided
  for a movie.
\end{itemize}

The scale and diversity of these data sets enables a robust evaluation
of our algorithm. The Mendeley, Echo Nest, and New York Times data
are sparse compared to Netflix. For example, we observe
only 0.001\% of all possible user-item ratings in Mendeley, while 1\%
of the ratings are non-zero in the Netflix data. This is partially a
reflection of large number of items relative to number users in these
data sets.

Furthermore, the intent signaled by an observed rating varies
significantly across these data sets. For instance, the Netflix data
set gives the most direct measure of stated preferences for items, as
users provide an star rating for movies they have watched. In
contrast, article click counts in the New York Times data are a less
clear measure of how much a user likes a given article---most articles
are read only once, and a click through is only a weak indicator of
whether the article was fully read, let alone liked. Ratings in the
Echo Nest data presumably fall somewhere in between, as the number of
times a user listens to a song likely reveals some indirect
information about their preferences.

As such, we treat each data set as a source of implicit feedback,
where an observed positive rating indicates that a user likes a
particular item, but the rating value itself is ignored. The Mendeley
data are already of this simple binary form. For the Echo Nest and New
York Times data, we consider any song play or article click as a
positive rating, regardless of the play or click count. As in previous
work, we consider an implicit version of the Netflix data where only 4
and 5 star ratings are retained as
observations~\cite{Paquet:2013p9197}.

\begin{figure*}[t!]
\centering
\includegraphics[width=0.8\textwidth]{newfigs/mean_diff_precision_at_20_by_user_percentile.pdf}\\
\includegraphics[width=0.8\textwidth]{newfigs/mean_diff_recall_at_20_by_user_percentile.pdf}\\
\caption{Predictive performance across users. The top and bottom plots show the
  mean difference in precision and recall to HPF at 20 recommendations,
  respectively, by user activity.}
\label{fig:precision_recall_by_user_activity}
\end{figure*}

{\bf Competing methods.} We compare Poisson factorization against an array of
competing methods:
\begin{itemize}
  \item {\bf NMF}: Non-negative Matrix
    Factorization~\cite{Lee:1999}. In NMF, user preferences and item
    attributes are modeled as non-negative vectors in a
    low-dimensional space. These latent vectors are randomly
    initialized and modified via an alternating multiplicative update
    rule to minimize the Kullback-Leibler divergence between the
    actual and modeled rating matrices.

  \item {\bf LDA}: Latent Dirichlet Allocation~\cite{Blei:2003b}. LDA
    is a Bayesian probabilistic generative model where user preferences
    are represented by a distribution over different topics, and each
    topic is a distribution over items. Interest and topic
    distributions are randomly initialized and updated using
    stochastic variational inference~\cite{Hoffman:2013} to
    approximate these intractable posteriors.

  \item {\bf MF}: Probabilistic Matrix Factorization with
    user and item biases. We use a variant of matrix factorization
    popularized through the Netflix Prize~\cite{Koren:2009}, where a
    linear predictor---comprised of a constant term, user activity and
    item popularity biases, and a low-rank interaction term---is fit
    to minimize the mean squared error between the predicted and
    observed rating values, subject to L2 regularization to avoid
    overfitting. Weights are randomly initialized and updated via
    stochastic gradient descent using the Vowpal Wabbit
    package~\cite{Weinberger:2009}. This corresponds to maximum
    a-posteriori inference under Probabilistic Matrix
    Factorization~\cite{Salakhutdinov:2008a}.
\end{itemize}

We note that while HPF and LDA take only the non-zero observed
ratings as input, traditional matrix factorization requires that we
provide explicit zeros in the ratings matrix as negative examples for
the implicit feedback setting. In practice, this amounts to either
treating all missing ratings as zeros (as in NMF) and down-weighting
to balance the relative importance of observed and missing
ratings~\cite{Hu:2008p9402}, or generating negatives by randomly
sampling from missing ratings in the training
set~\cite{Gantner:2012p9364,Dror:2012a,Paquet:2013p9197}.  We take the
latter approach for computational convenience, employing a
popularity-based sampling scheme: we sample users by activity---the
number of items rated in the training set---and items by
popularity---the number of training ratings an item received to
generate negative examples.\footnote{We also compared this to a
  uniform random sampling of negative examples, but found that the
  popularity-based sampling performed better.} 

Finally, we note a couple of candidate algorithms that failed to scale
to our data sets. The fully Bayesian treatment of the Probabilistic
Matrix Factorization~\cite{Salakhutdinov:2008}, uses a MCMC algorithm
for inference. The authors~\cite{Salakhutdinov:2008} report that a
single Gibbs iteration on the Netflix data set with 60 latent factors,
requires ~30 minutes, and that they throw away the first 800
samples. This implies at least 16 days of training, while the HPF
variational inference algorithm converges within 13 hours on the
Netflix data. Another alternative, Bayesian Personalized Ranking
(BPR)~\cite{Rendle:2009p9243,Gantner:2012p9364}, optimizes a
ranking-based criteria using stochastic gradient descent. The
algorithm performs an expensive bootstrap sampling step at each
iteration to generate negative examples from the vast set of
unobserved. We found time and space constraints to be prohibitive when
attempting to use BPR with the data sets considered here.

%% attempted to get SIGIR code, but failed

{\bf Evaluation.} Prior to training any models, we randomly select
20\% of ratings in each data set to be used as a held-out test set
comprised of items that the user has consumed. Additionally, we set
aside 1\% of the training ratings as a validation set and use it to
determine algorithm convergence and to tune free parameters. We used
the HPF settings described in \mysec{inference} across all
data sets, and set the number of latent components $K$ to $100$.

During testing, we generate the top $M$ recommendations for each user
as those items with the highest predictive score under each
method. For each user, we compute a variant of precision-at-$M$ that
measures the fraction of relevant items in the user's top-$M$
recommendations. So as not to artificially deflate this measurement
for lightly active users who have consumed fewer than $M$ items, we
compute {\it normalized} precision-at-$M$, which adjusts the
denominator to be at most the number of items the user has in the
test set. Likewise, we compute recall-at-$M$, which captures the
fraction of items in the test set present in the top $M$
recommendations.

%% \begin{equation*}
%%   \textrm{NPrec}@M = \frac{|~\textrm{relevant items at $M$ recommendations}~|}
%%          {\mathrm{min}(M, |~\textrm{relevant items}~|)}
%% \end{equation*}

\myfig{precision_recall} shows the normalized mean precision at 20
recommendations for each method and data sets. We see that HPF
outperforms other methods on all data sets by a sizeable margin---as
much as 8 percentage points. Poisson factorization provides
high-quality recommendations---a relatively high fraction of items
recommended by HPF are found to be relevant, and many relevant items
are recommended. While not shown in these plots, the relative
performance of methods within a data set is consistent as we vary the
number of recommendations shown to users. We also note that while
Poisson factorization dominates across all of these data sets, the
relative quality of recommendations from competing methods varies
substantially from one data set to the next. For instance, LDA
performs quite well on the Echo Nest data, but fails to beat classical
matrix factorization for the implicit Netflix data set.

We also study precision and recall as a function of user activity to
investigate how performance varies across users of different types. In
particular, \myfig{precision_recall_by_user_activity} shows the mean
difference in precision and recall to HPF, at 20 recommendations, as
we look at performance for users of varying activity, measured by
percentile. For example, the 10\% mark on the x-axis shows mean
performance across the bottom 10\% of users, who are least active; the
90\% mark shows the mean performance for all but the top 10\% of most
active users. Here we see that Poisson factorization outperforms other
methods for users of all activity levels---both the ``light'' users
who constitute the majority, and the relatively few ``heavy'' users
who consume more---for all data sets.

%% recys-prem !!! removed paragraph below on higher levels of user activity

%% For the New York Times and Netflix data, we see that higher levels of
%% user activity enable us to better estimate user preferences and
%% improve the quality of recommendations, as measured by mean normalized
%% precision. For the Mendeley and Echo Nest data, however, we see a
%% decline in normalized precision for the heavy users.  One possible
%% reason for this decline is that these data sets contain many more
%% items than users, and this makes prediction difficult.  For example,
%% heavy users in the music data may have more diverse taste than the
%% light users.

%%
%% Presumably this difference is due to the large catalogues of
%% items relative to the number of users in these data sets,
%% comparatively. (??? More nicheness of users???) 
%%

%% recys-prem !!! removed paragraph below on higher levels of user activity

%% We also note that the decrease in mean recall as we consider
%% increasingly active sets of users is inevitable, as these individuals
%% have a larger set of relevant items in the test set than their less
%% active counterparts. 

% \myfig{precision_recall} shows that HPF outperforms BPF on the
% Mendeley, New York Times and Echo Nest data sets in terms of mean
% normalized precision and mean recall. Knowing how many articles a user
% has read in one component helps predict her consumption on other
% components. However, we note that HPF and BPF have similar mean
% performance on the Netflix data sets, with BPF performing slightly
% better on users with low activity. This suggests that there is less
% sharing of statistical strength among components for any given users
% on these movie datasets.

{\bf Exploratory analysis.} The fitted model can be explored to discover latent
structure among items and users and to confirm that the model is capturing the
components in the data in a reasonable way. For example, in \myfig{components}
we illustrate the components discovered by our algorithm on the news articles
in the New York Times. The illustration shows the top items---items sorted in
decreasing order of their expected weight $\beta_i$---from three of the 100
components discovered by our algorithm. From these, we see that learned
components both cut across and differentiate between conventional topics and
categories. We find that multiple business-related topics (e.g., self help and
personal finance) comprise separate components, whereas other articles that
appear across different sections of the newspaper (e.g., business and regional
news) are unified by their content (e.g., airplanes).

%% In \myfig{movielens-illustration} we show a subset of the highly rated
%% movies of a user from the MovieLens data
%% set~\cite{Herlocker:1999}. The top 15 movies recommended to this user
%% using the trained HPF model, are also shown. The user's ratings are
%% for primarily drama movies. We movies HPF recommends closely resemble
%% the types of drama movies she is interested in, for example,
%% ``Children's drama'' or ``War drama''. The expected user's $K$-vector
%% of weights $\theta_u$, inferred by our algorithm, is shown in
%% \myfig{movielens-illustration}. In our analysis, $K$ was set to
%% 100. The $\theta_u$ are not sparse because the user's views span a
%% range of movies in the small data set.

\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{./figures/attributes-nyt.pdf}
\caption{The top 10 items by the expected weight $\beta_i$ from three
  of the 100 components discovered by our algorithm for the New York
  Times data set.}
\label{fig:components}
\end{figure}
