
\begin{comment}
  * details of datasets
    * variety of activities
    * different balance of user, item, density
    * different meaning to ratings
    * implicit vs explicit (be clear about netflix)
  * preprocessing
  * baselines
    * NMF
    * LDA
    * MF (sampling schemes)
    * train/test methology
  * performance
    * overall dominance (draw distinction btwn HPF and BPF?)
    * no need to sample (time savings)
    * default hyperparameters vs. grid search
    * failing of bpr
    * variation by user activity (helping head vs. tail)
  * exploratory
\end{comment}

\begin{figure*}[t!]
\centering
\includegraphics[width=\textwidth]{figures/mean_precision_at_20.pdf}\\
\includegraphics[width=\textwidth]{figures/mean_recall_at_20.pdf}\\
\caption{Predictive performance on data sets. The top and bottom plots
  show normalized mean precision and mean recall at 20
  recommendations, respectively. While baseline performance varies
  across data sets, HPF and BPF consistently outperform competing
  methods.}
\label{fig:precision_recall}
\end{figure*}


\section{Empirical Study}
We evaluate the performance of the Hierarchical Poisson Factorization
(HPF) algorithm and its non-hierarchical variant (BPF) on a variety of
large-scale data sets for the purpose of recommending new items to
users. We first discuss details of each data set and of competing
recommendation methods. In particular, we note the superior
performance and computational efficiency of Poisson Factorization in
comparison to baseline methods. We conclude with an exploratory
analysis of the preferences and attributes inferred by HPF.

{\bf Data Sets.} We study the HPF algorithm in Figure~\ref{fig:batch}
on several data sets of user behavior that contain both implicit
and explicit feedback:
\begin{itemize}
\item The {\bf Mendeley} data set~\cite{Jack:2010} of scientific
  articles is a binary matrix of 80,000 users and 260,000 articles,
  where each observation corresponds to the presence of a given
  article in a particular user's online library.
\item The {\bf Echo Nest} music data set~\cite{Bertin-Mahieux:2011}
  consists of 1 million users, 385,000 distinct songs and 48 million
  (user, song, play count) triplets.
\item The {\bf New York Times} data set with 1,615,675 users, 103,390
  articles, and 80 million (user, article, view count)
  observations.
\item The {\bf Netflix} data set~\cite{Koren:2009} consists of 480,000
  users, 17,770 movies and 100 million ratings, where each observation
  is an explicit rating (from 1 to 5 stars) that a user provided for
  the given movie.
  discarded~\cite{Paquet:2013p9197}.
\end{itemize}

The scale and diversity of these data sets enables a robust evaluation
of our algorithm. The Mendeley, Echo Nest, and New York Times data
sets are sparse in comparison to the movie data. For example, we
observe only 0.001\% of all possible user-item ratings in Mendeley,
while 1\% of the ratings are non-zero in the Netflix data. This is
partially a reflection of large number of articles relative to number
users in the Mendeley data.

Furthermore, the intent signaled by an observed rating varies
significantly across these data sets. For instance, the Netflix data
set gives the most direct measure of stated preferences for items, as
users provide an explicit star rating for movies they have watched. In
contrast, article click counts in the New York Times data are a less
clear measure of how much a user likes a given article---most articles
are read only once, and a click through is only a weak indicator of
whether the article was fully read, let alone liked. Ratings in the
Echo Nest data presumably fall somewhere in between, as the number of
times a user listens to a song likely reveals some indirect
information about their preferences.

As such, we treat each data set as a source of implicit feedback,
where an observed positive rating indicates that a user likes a
particular item, but the rating value is ignored. The Mendeley data
are already of this simple binary form. For the Echo Nest and New York
Times data, we consider any song play or article click as a positive
rating, regardless of the play or click count. We also consider two
versions of the Netflix data---the original, explicit ratings, and an
implicit version in which only 4 and 5 star ratings are retained as
observations~\cite{Paquet:2013p9197}.

{\bf Pre-processing and training.} We now describe how we the
pre-process the input data, train the model, and assess the predictive
accuracy on a held-out set.

Prior to training, we randomly select 20\% of ratings in each data set
to be used as a held-out test set comprised of items that the user
has consumed. During training, these test set observations are treated
as zeros. Additionally, we set aside 1\% of the training ratings as a
validation set and use it to determine the convergence of our
algorithm.

During training, the input to our algorithm is the count data, for
example, the movie ratings or the play count of a song. Notice that
for the Mendeley data set, the input is binary. During training, we
fix the shape and rate hyperparameters. This gives good performance on
our validation set. We find that the algorithm is insensitive to small
changes in the hyper-parameters.

We terminate the training process when the HPF algorithm
converges. The convergence is measured by computing the prediction
accuracy on our validation set. We approximate the probability that a
user consumed an item using the variational approximations to
posterior expectations of $\theta_u$ and $\beta_i$, and compute the
average predictive log likelihood of the validation ratings. The HPF
algorithm stops when the change in log likelihood is less than
0.0001\%.

\begin{figure*}[t!]
\centering
\includegraphics[width=\textwidth]{figures/mean_precision_at_20_by_user_percentile.pdf}\\
\includegraphics[width=\textwidth]{figures/mean_recall_at_20_by_user_percentile.pdf}\\
\caption{Predictive performance across users. The top and bottom plots
  show normalized mean precision and mean recall at 20
  recommendations, respectively, by user activity.}
\label{fig:precision_recall_by_user_activity}
\end{figure*}


{\bf Baselines.} We compare our performance against traditional matrix
factorization (MF). Ratings are modeled as
\begin{equation*}
  y_{ui} = c + a_u + b_i + \theta_u^\top \beta_i,
\end{equation*}
where $c$ denotes a global intercept term, $a_u$ captures the relative
activity of the $u$-th user, and $b_i$ accounts for the popularity of
the $i$-th item. The final term quantifies interactions between a user
and item via their K-dimensional latent factors, with $\theta_u$ specifying
the user's interests and $\beta_i$ describing the item's attributes. The
model is fit by stochastic gradient descent using the open source
Vowpal Wabbit package~\cite{Weinberger:2009} to minimize squared error
between predicted and actual ratings.

We note that while HPF takes only the non-zero observed ratings as
input, traditional matrix factorization requires that we provide
explicit zeros in the ratings matrix as negative examples. In
practice, this amounts to either treating all missing ratings as zeros
and down-weighting to balance the relative importance of observed and
missing ratings~\cite{Hu:2008p9402}, or generating negatives by
randomly sampling from missing ratings in the training
set~\cite{Dror:2012a}.  We take the latter approach for computational
convenience, employing two popular sampling schemes. In the first,
denoted MF-UNI, we sample a fixed number of negative examples
uniformly at random for each user such that there are approximately
the same number of positive and negative examples. In the second,
termed MF-POP, we sample users by activity---the number of items rated
in the training set---and items by popularity---the number of training
ratings an item received.

For MF, we cross-validate over the gradient update step size, the
strength of an $L_2$-regularization term across all weights, and the
number of passes over the training data. This results in a reasonably
expensive grid search, from which we select
the model that performs best on the validation set for each data set.

{\bf Results and analysis.} During testing, we generate the top $M$
recommendations for each user as those items with the highest
predictive score in \myeq{score}. The ranked list of items predicted
for each user includes items in the test set, as well as items in the
training set that were zeros. We compute normalized precision-at-$M$,
which measures the fraction of the top $M$ recommendations present in
the test set, varying $M$ from 10 to 100 items. Likewise, we compute
recall-at-$M$, which captures the fraction of items in the test set
present in the top $M$ recommendations.

XXX explain normalized precision

\myfig{precision_recall} shows the normalized mean precision at 20
recommendations for each method and data sets. We see that HPF
outperforms other methods on all data sets by a sizeable margin---as
much as 8 percentage points. Likewise, HPF shows similar gains in
recall over traditional MF approaches, indicated in
\myfig{recall_by_M}. A relatively high fraction of items recommended
by HPF are found to be relevant, and many relevant items are
recommended.

We also study precision as a function of user activity to investigate
for which kinds of users the algorithm performs
well. \myfig{precision_recall_by_user_activity} highlights the results
in further detail, showing the normalized mean precision and mean
recall at 20 recommendations for users of varying activity. HPF
provides increasingly better performance for more active users.

As expected, we see that HPF provides increasingly better
recommendations for more active users, and that HPF outperforms MF on
the Netflix, MovieLens, and Echo Nest data sets by this measure as
well as mean precision across all users.


{\bf Exploratory analysis.} The fitted model can be explored to
discover latent structure among items and users and to confirm that
the model is capturing the components in the data in a reasonable
way. For example, in \myfig{components} we illustrate the components
discovered by our algorithm on the scientific articles in Mendeley,
movies in Neflix and articles in the New York Times. For each data
set, the illustration shows the top 10 items---items sorted in
decreasing order of their expected weight $\beta_i$---from three of
the 100 components discovered by our algorithm. These components
naturally organize the movies and articles, and enable recommendation
of new items to the user.

In \myfig{movielens-illustration} we show a subset of the highly rated
movies of a user from the MovieLens data
set~\cite{Herlocker:1999}. The top 15 movies recommended to this user
using the trained HPF model, are also shown. The user's ratings are
for primarily drama movies. We movies HPF recommends closely resemble
the types of drama movies she is interested in, for example,
``Children's drama'' or ``War drama''. The expected user's $K$-vector
of weights $\theta_u$, inferred by our algorithm, is shown in
\myfig{movielens-illustration}. In our analysis, $K$ was set to
100. The $\theta_u$ are not sparse because the user's views span a
range of movies in the small data set.

\begin{figure*}
\centering
\includegraphics[width=\textwidth]{./figures/components.pdf}
\caption{The top 10 items by the expected weight $\beta_i$ from three
  of the 100 components discovered by our algorithm.}
\label{fig:components}
\end{figure*}
