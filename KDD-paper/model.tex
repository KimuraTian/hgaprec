\section{Poisson Recommendation}
\label{sec:model}

% !!! review the Gamama and Poisson near the generative process.
% !!! make n_u and n_i the sums
% !!! somewhere we need to make clear how our notation works

In this section we describe the Poisson factorization model for
recommendation, and discuss its statistical properties.

We are given data about users and items, where each user has consumed
and possibly rated a set of items.  The observation $y_{ui}$ is the
rating that user $u$ gave to item $i$, or zero if no rating was given.
(In so-called ``implicit'' consumer data, $y_{ui}$ equals one if user
$u$ consumed item $i$ and zero otherwise.)  User behavior data, such
as purchases, ratings, clicks, or views, are typically sparse.  Most
of the values of the matrix $y$ are zero.

We model the data with factorized Poisson
distributions~\cite{Canny:2004}.  We represent each item $i$ as a
vector of $K$ latent attributes $\beta_i$ and represent each user $u$
as a vector of $K$ latent preferences $\beta_u$.  These vectors are
sparse and non-negative.  The observations $y_{ui}$ are modeled with a
Poisson, parameterized by the inner product of the user preferences
and item attributes, $y_{ui} \sim \poisson(\theta_u^\top \beta_i)$.
This is a variant of probabilistic matrix
factorization~\cite{Salakhutdinov:2008a} but where each user and
item's weights are positive~\cite{Lee:1999} and where the Poisson
replaces the Gaussian.

Beyond the basic data generating distribution, we place Gamma priors
on the latent attributes and latent preferences, which encourage the
model towards sparse representations.  Furthermore, we place
additional priors on the user and item-specific scale parameter of
those Gammas, which controls the average size of the representation.
This hierarchical structure allows us to capture the diversity of
users, some tending to consume more than others, and the diversity of
items, some being more popular than others.

Putting this together, the generative process of the hierarchical
Poisson factorization model (HPF) is as follows:
\begin{enumerate*}
\item For each user $u$:
  \begin{enumerate*}
  \item Sample activity $\xi_u \sim \gam(a', a'/b')$.
  \item For each component $k$, sample preference $$\theta_{uk} \sim
    \gam(a, \xi_u).$$
  \end{enumerate*}

\item For each item $i$:
  \begin{enumerate*}
    \item Sample popularity $\eta_i \sim \gam(c', c'/d')$.
    \item For each component $k$, sample attribute
      $$\beta_{ik} \sim \gam(c, \eta_i).$$
  \end{enumerate*}

\item For each user $u$ and item $i$, sample rating
  $$y_{ui} \sim
  \poisson(\theta_u^\top \beta_i).$$
\end{enumerate*}
This process describes the statistical assumptions behind the model.
Note that we also study a sub-class of the HPF where we fix the rate
parameters for all users and items to the same pair of
hyperparameters. We call this model Bayesian Poisson Factorization
(BPF).

The key computational problem is posterior inference, which is akin to
``reversing'' the generative process.  Given a user behavior matrix,
we want to estimate the conditional distribution of the latent
per-user and per-item structure, $p(\theta_{1:N} \beta_{1:M} \g y)$.
The posterior is the key to recommendation.  We estimate the posterior
expectation of each user's preferences, each items attributes and,
subsequently, form predictions about which unconsumed items each user
will like.  We discuss our posterior inference algorithm in detail in
\mysec{inference}.  Figure 1 illustrates posterior estimates of
$\beta$ and $\theta$ on Netflix data.

% !!! I AM HERE

% jmh: the below seemed out of place, and is repeated in the algorithm
% section just before empirical results, which seems like a better
% spot

%% \subsection{The Posterior distribution}
%% The posterior distribution of the latent variables $p(\theta, \beta,
%% \xi, \eta \g y)$ embeds users and items in a latent space of
%% $K$-dimensional positive vectors. 

%% We use the HPF to recommend items to users by predicting which of the
%% unconsumed items each will like.  We rank each user's unconsumed items
%% by their posterior expected Poisson parameters,
%% \begin{equation}
%%   \label{eq:score}
%%   \textrm{score}_{ui} = \E[\theta_u^\top \beta_i \g y].
%% \end{equation}
%% This amounts to asking the model to rank by probability which of the
%% presently unconsumed items each user will likely consume in the
%% future.

A good collaborative filtering model must capture two basic features
of user and item interactions. First, for any given observation
$y_{ui}$, we expect only a small fraction of the user $u$'s factors
and the item $i$'s factors to be relevant. This favors sparsity in the
weights. Second, there are systematic patterns in the data due to
universally popular items and highly active users. The literature on
recommendation systems suggests that a good model must capture such
heterogeneity across users and items~\cite{Koren:2009}. We therefore
extend the Poisson factorization model with a hierarchical prior
structure. We describe our model, and then discuss how it captures the
above properties.

The HPF model is more powerful than the BPF because it allows for the
sharing of statistical strength. In the HPF, for example, learning
about the number of items consumed by one set of users provides weak,
indirect evidence relevant to other users consumption. This can
improve predictions, especially for users with little activity. A
similar argument favors hierarchy over item popularities.

\subsection{Statistical properties of the HPF model}
With the modeling details in place, we highlight statistical
properties of HPF that provide advantages over classical matrix
factorization, and form the basis for scalable inference.

{\bf Sparse factors}.
To enforce sparse user and item factors, the model places a \gam~prior
on the independent weight components. The \gam~rate parameters capture
the magnitude of user $u$'s activity and item $i$'s popularity,
respectively. The \gam~prior on the rate parameter $\xi_u$ governing
each user $u$ or $\eta_i$ governing each item $i$ accommodates
heterogeneity in user activity and item popularity. The prior on
$\xi_u$ captures the uncertainty around user $u$'s consumption rate or
activity; the prior on $\eta_i$ captures the uncertainty around item
$i$'s popularity among users. Notice that we fix the shape parameters
and share them across users and items.

Parameterized in terms of a shape parameter $a$ and an inverse scale
parameter $\frac{a}{b}$, the Gamma distribution has a mean of $b$ and
a variance of $\frac{b^2}{a}$. For a small shape $a$, most of the
weights will be close to zero, and only a few will be large, resulting
in a sparse representation.


{\bf Modeling the long-tail}.
Figure 3 shows the distributions of user activity on large, real-world
ratings data sets. These distributions are long-tailed: while most
users consume a handful few items, a few ``tail users'' consume
thousands of items. Under the BPF and the HPF, the marginal
distribution of each user's ratings, with the item weights held fixed,
is a negative binomial.  Let $y_{u} = \sum_{i} y_{ui}$ be the sum of
the ratings for user $u$.  Since each of the terms in the sum is a
Poisson distribution with rate $\theta_u^\top \beta_i$, the sum is
itself a Poisson random variable~\cite{Johnson:2005}.
\begin{equation}
  y_u \sim \poisson\left(\theta_u^\top (\textstyle \sum_{i} \beta_{i})\right).
\end{equation}
Holding the item weights fixed, note that the rate of this Poisson is
a sum of scaled Gamma random variables $\beta_{ik} \theta_{uk}$, which
is itself a Gamma variable~\cite{Norman:1994}.  Thus the marginal
distribution of $y_u$ is from an integrated Gamma-Poisson
distribution, which is a negative binomial~\cite{Gelman:1995}.

The negative binomial is a long-tailed distribution that is known to
fit user activity well~\cite{Goodhardt:1984,Dunning:1993}.  Figure 3
shows that the HPF provides good fits to the distribution of user
activity on real-world data.  Classical matrix factorization is based
on Gaussian likelihoods, and the marginal distribution of total user
ratings is a Gaussian.  The Gaussian is a poor choice for modeling
long-tailed distributions. Further, it places probability mass on
negative numbers, a setting that never occurs. While we have focused
on user activity here, the same is true for the marginal distribution
of item popularities under Poisson factorization.

{\bf Down-weighting the effect of zeros.}
An added benefit of appropriately modeling the skew in user activity
and item popularity is that the model implicitly down-weights the
contribution of the items that each user did not consume. The reason
is that the model has two ways of explaining the observation that a
user has not consumed an item: either she is not interested in it or
she would be but has already consumed her allotted negative-binomial
number.  In contrast, a user that consumes an item must be interested
in it.  Thus, the model benefits more from making a consumed user/item
pair more similar than making an unconsumed user/item pair less
similar.

Classical matrix factorization is based on Gaussian likelihoods (i.e.,
squared loss), which gives equal weight to consumed and unconsumed
items.  Consequently, when faced with a sparse matrix and implicit
feedback, i.e., binary consumption data, matrix factorization places
more total emphasis on the unconsumed user/item pairs.  To address
this, researchers have patched the model in complex ways, for example,
by including per-observation confidences~\cite{Koren:2009} or
considering all zeroes to be hidden variables~\cite{Paquet:2013p9197}.
Poisson factorization more naturally solves this problem by better
capturing each user's rate of consumption.

As an example, consider two similar science fiction movies, ``Star
Wars'' and ``The Empire Strikes Back'', and consider a user who has
seen one of them.  The Gaussian model pays an equal penalty for making
the user similar to these items as it does for making the user
different from them---with quadratic loss, seeing ``Star Wars'' is
evidence for liking science fiction, but not seeing ``The Empire
Strikes Back'' is evidence for disliking it.  The Poisson model,
however, will prefer to bring the user's latent weights closer to the
movies' weights because it favors the information from the user
watching ``Star Wars''. Further, because the movies are similar, this
increases the Poisson model's predictive score that a user who watches
``Star Wars'' will also watch ``The Empire Strikes Back''.

{\bf Fast inference with sparse matrices.}
The likelihood of the observed data under the BPF and the HPF depends
only on the consumed items, that is, the non-zero elements of the
user/item matrix $y$.  Given the latent variables $\theta_u$ and
$\beta_i$, the Poisson distribution of $y_{ui}$ is
\begin{equation}
  p(y_{ui} \g \theta_u, \beta_i) =
  \left(\theta_u^\top \beta_i\right)^y
  \exp\left\{-\theta_u^\top \beta_i \right\} / y_{ui}!
\end{equation}
Recall the elementary fact that $0! = 1$.  The log probability of the
complete matrix $y$ is
\begin{align}
  \log p(y \g \theta, \beta) =
  & \left(\textstyle \sum_{\{y_{ui} > 0\}}
    y_{ui} \log (\theta_u^\top \beta_i) - \log y_{ui}!
  \right) \\
  & -
  \left(\textstyle\sum_{u} \theta_u\right)^\top \left(\textstyle
    \sum_{i} \beta_i\right).
\end{align}
That this likelihood depends only on the non-zero elements facilitates
inference with sparse matrices.  In contrast, classical matrix
factorization methods, especially when applied to
massive data sets, must address the zeros either through
sub-sampling~\cite{Dror:2012a} or approximation~\cite{Hu:2008p9402}.

% dmb: above, maybe add a comment above that this is a heavy-tailed
% realistic distribution for consumer data.  cite goodhardt 1984
% again.

