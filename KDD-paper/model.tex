\section{Poisson Recommendation}
\label{sec:model}

% !!! review the Gamama and Poisson near the generative process.
% !!! make n_u and n_i the sums
% !!! somewhere we need to make clear how our notation works

We are given data about users and items.  Each user has consumed and
possibly rated a set of items.  The observation $y_{ui}$ is the rating
that user $i$ gave to item $j$, or zero if no rating was given.  (In
simple consumer data, $y_{ui}$ equals one if user $u$ consumed item
$i$ and zero otherwise.)  User behavior data, such as purchases,
ratings, clicks, ``likes'', or ``check ins'', are typically sparse.
Most of the values of the matrix $y$ are zero.

We model this data with factorized Poisson
distributions~\cite{Canny:2004}. Each user $u$ and each item $i$ is
associated with a $K$-vector of positive weights, $\theta_u$ and
$\beta_i$ respectively.  (The number $K$ is fixed in advance.)  The
user/item observation $y_{ui}$ is modeled with a Poisson parameterized
by the inner product of the user's and item's weights $y_{ui} \sim
\poisson(\theta_u^\top \beta_i)$.  This is a variant of probabilistic
matrix factorization~\cite{Salakhutdinov:2008a} but where each user
and item's weights are positive~\cite{Lee:1999} and where the Poisson
likelihood replaces the Gaussian likelihood.

A good collaborative filtering model must capture two basic features
of user and item interactions. First, for any given observation
$y_{ui}$, we expect only a small fraction of the user $u$'s factors
and the item $i$'s factors to be relevant. This favors sparsity in the
weights. Second, there are systematic patterns in the data due to
universally popular items and frequently consuming users. The
literature on recommendation systems suggests that a good model must
capture such heterogeneity across users and items~\cite{Koren:2009}. We
therefore extend the Poisson factorization model with a hierarchical
prior structure. We describe our model, and then discuss how it
captures the above properties.

% dmb: jake cites something below
% dmb: also cite "The Dirichlet", old marketing paper

The generative process of the hierarchical Poisson factorization model
(HPF) is as follows:
\begin{enumerate}
\item For each user $u$, choose activity
  \begin{equation*}
    \xi_u \sim \gam(a', a'/b').
  \end{equation*}
\item For each user $u$ and each component $k$, choose weight
  \begin{equation*}
    \theta_{uk} \sim \gam(a, \xi_u).
  \end{equation*}
\item For each item $i$, choose popularity
  \begin{equation*}
    \eta_i \sim \gam(c', c'/d').
  \end{equation*}
\item For each item $i$ and each component $k$, choose weight
  \begin{equation*}
    \beta_{ik} \sim \gam(c, \eta_i).
  \end{equation*}
\item For each user $u$ and item $i$, choose rating
  \begin{equation*}
    y_{ui} \sim \poisson(\theta_u^\top \beta_i).
  \end{equation*}
\end{enumerate}
To enforce sparse weights, the model places a \gam~prior on the
independent weight components. $\gam(x;a,\frac{a}{b})$ parameterized
in terms of a shape parameter $a$ and an inverse scale parameter
$\frac{a}{b}$, has a mean of $b$ and a variance of
$\frac{b^2}{a}$. For a small shape $a$, most of the weights will be
close to zero, and only a few will be large, resulting in a sparse
representation.

The \gam~rate parameters capture the magnitude of user $u$'s activity
and item $i$'s popularity, respectively.  To capture heterogeneity in
user activity and item popularity, we place a \gam~prior on the rate
parameter $\xi_u$ governing each user $u$ or $\eta_i$ governing each
item $i$. The prior on $\xi_u$ captures the uncertainty around user
$u$'s consumption rate or activity; the prior on $\eta_i$ captures the
uncertainty around item $i$'s popularity among users. Notice that we
fix the shape parameters and share them across users and items.

% dmb: jake cites something below

We also study a sub-class of the HPF where we fix the rate parameters
for all users and items to the same pair of hyperparameters. We call
this model the Bayesian Poisson Factorization (BPF) model.

The HPF model is more powerful than the BPF because it allows for the
sharing of statistical strength. In the HPF, for example, learning
about the number of items consumed by one set of users provides weak,
indirect evidence relevant to other users consumption. This can
improve predictions, especially for users with little activity. A
similar argument favors hierarchy over item popularities. 

Both the HPF and the BPF are conjugate models, resulting in a
computationally efficient inference algorithm.

\subsection{The Posterior distribution}
The posterior distribution of the latent variables $p(\theta, \beta,
\xi, \eta \g y)$ embeds users and items in a latent space of
$K$-dimensional positive vectors. 

We use the HPF to recommend items to users by predicting which of the
unconsumed items each will like.  We rank each user's unconsumed items
by their posterior expected Poisson parameters,
\begin{equation}
  \label{eq:score}
  \textrm{score}_{ui} = \E[\theta_u^\top \beta_i \g y].
\end{equation}
This amounts to asking the model to rank by probability which of the
presently unconsumed items each user will likely consume in the
future.

\subsection{Statistical properties of the HPF model}
With the modeling details in place, we highlight statistical
properties of the model that provide advantages over classical matrix
factorization, and form the basis for scalable inference.

\begin{enumerate}
\item {\bf Modeling the long-tailed distribution of user activity}.
  Figure 3 shows the distributions of user activity on large,
  real-world user/item matrices. These distributions are highly
  skewed, with significant number of users exhibiting much higher
  activity than most users.

Under the BPF and the HPF, the marginal distribution of each user's
ratings, with the item weights held fixed, is a negative binomial.
Let $y_{u} = \sum_{i} y_{ui}$ be the sum of the ratings for user $u$.
Since each of the terms in the sum is a Poisson distribution with rate
$\theta_u^\top \beta_i$, the sum is itself a Poisson random
variable~\cite{Johnson:2005}.
\begin{equation}
  y_u \sim \poisson\left(\theta_u^\top (\textstyle \sum_{i} \beta_{i})\right).
\end{equation}
Holding the item weights fixed, note that the rate of this Poisson is
a sum of scaled Gamma random variables $\beta_{ik} \theta_{uk}$, which
is itself a Gamma variable~\cite{Norman:1994}.  Thus the marginal
distribution of $y_u$ is from an integrated Gamma-Poisson
distribution, which is a negative binomial~\cite{Gelman:1995}.

The negative binomial is a long-tailed distribution that is known to
fit user activity well~\cite{Goodhardt:1984,Dunn1983}.  Figure 3 shows
that the HPF provide good fits to the distribution of user activity on
real-world data.  Classical matrix factorization is based on Gaussian
likelihoods, and the marginal distribution of total user ratings is
also a Gaussian.  The symmetric Gaussian assumption is a poor choice
for modeling long-tailed distributions, and places probability mass on
negative numbers, a setting that never occurs.


\item {\bf Down-weighting the effect of zeros.}
The marginal distribution of $y_u$ is a negative binomial, and this
forces it to down-weight the contribution of the items that each user
did not consume. The reason is that the model has two ways of
explaining that a user does not consume an item: either she is not
interested in it or she would be but has already consumed her allotted
negative-binomial number.  In contrast, a user that consumes an item
must be interested in it.  Thus, the model benefits more from making a
consumed user/item pair more similar than making an unconsumed
user/item pair less similar.

Classical matrix factorization is based on Gaussian likelihoods (i.e.,
squared loss), which gives equal weight to consumed and unconsumed
items.  Consequently, when faced with a sparse matrix and implicit
feedback, i.e., binary consumption data, matrix factorization places
more total emphasis on the unconsumed user/item pairs.  To address
this, researchers have patched the model in complex ways, for example,
by including per-observation confidences~\cite{Koren:2009} or
considering all zeroes to be hidden variables~\cite{Paquet:2013p9197}.
Poisson factorization more naturally solves this problem by better
capturing each user's rate of consumption.

As an example, consider two similar science fiction movies, ``Star
Wars'' and ``The Empire Strikes Back'', and consider a user who has
seen one of them.  The Gaussian model pays an equal penalty for making
the user similar to these items as it does for making the user
different from them---with quadratic loss, seeing ``Star Wars'' is
evidence for liking science fiction, but not seeing ``The Empire
Strikes Back'' is evidence for disliking it.  The Poisson model,
however, will prefer to bring the user's latent weights closer to the
movies' weights because it favors the information from the user
watching ``Star Wars''. Further, because the movies are similar, this
increases the Poisson model's predictive score that a user who watches
``Star Wars'' will also watch ``The Empire Strikes Back''.

\item {\bf Fast inference with sparse matrices.}
The likelihood of the observed data under the BPF and the HPF depends
only on the consumed items, that is, the non-zero elements of the
user/item matrix $y$.  Given the latent variables $\theta_u$ and
$\beta_i$, the Poisson distribution of $y_{ui}$ is
\begin{equation}
  p(y_{ui} \g \theta_u, \beta_i) =
  \left(\theta_u^\top \beta_i\right)^y
  \exp\left\{-\theta_u^\top \beta_i \right\} / y_{ui}!
\end{equation}
Recall the elementary fact that $0! = 1$.  The log probability of the
complete matrix $y$ is
\begin{align}
  \log p(y \g \theta, \beta) =
  & \left(\textstyle \sum_{\{y_{ui} > 0\}}
    y_{ui} \log (\theta_u^\top \beta_i) - \log y_{ui}!
  \right) \\
  & -
  \left(\textstyle\sum_{u} \theta_u\right)^\top \left(\textstyle
    \sum_{i} \beta_i\right).
\end{align}
That this likelihood depends only on the non-zero elements facilitates
inference with sparse matrices.  In contrast, classical matrix
factorization methods, especially when applied to
massive data sets, must address the zeros either through
sub-sampling~\cite{Dror:2012a} or approximation~\cite{Hu:2008p9402}.

\item {\bf }

\end{enumerate}

% dmb: above, maybe add a comment above that this is a heavy-tailed
% realistic distribution for consumer data.  cite goodhardt 1984
% again.

