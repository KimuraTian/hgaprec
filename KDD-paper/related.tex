\section{Related work}
The roots of Poisson factorization come from nonnegative matrix
factorization~\cite{Lee:1999}, where the objective function is
equivalent to a factorized Poisson likelihood.  The original NMF
update equations have been shown to be an expectation-maximisation
(EM) algorithm for maximum likelihood estimation of a Poisson model
via data augmentation~\cite{Cemgil:2009}.

Placing a Gamma prior on the user weights gives the GaP
model~\cite{Canny:2004}, which was developed as an alternative text
model to latent Dirichlet allocation (LDA)~\cite{Blei:2003b}. Placing
a Gamma prior on both the user and item weights gives the
Probabilistic Factor Model (PFM)~\cite{Ma:2011}.

A difference between our treatment and GaP is that GaP fits the item
weights with maximum likelihood via the expectation maximization
algorithm. We approximate the full posterior with variational
inference. Further, our model places a hierarchical prior structure
using Gamma priors on weights, and Gamma priors on user and item rate
parameters.  Placing priors on both sets of weights further
regularizes the model and lets us use the same inferential machinery
in both user-space and item-space. 

We note that GaP was developed as an alternative to LDA. In Appendix
A, we show that LDA can be reinterpreted as an instance of Poisson
factorization where we condition on the user counts and use an
alternative prior on the item weights. This relationship has also been
explored in ~\cite{Inouye:2014}.

Although the PFM~\cite{Ma:2011} model is the same as the BPF, the PFM
was aimed at a particular application---that of recommending
web-sites---while we demonstrate improved performance on a variety of
recommendation tasks. Further, we study why Poisson factorization
based models outperform others in recommendation. We also extended the
BPF with a hierarchical prior structure to capture the skew in user
activity and item popularity. Capturing these biases is important for
predictive performance~\cite{Koren:2009}. Again, we approximate the
full posterior with variational inference, while PFM uses Maximum a
posteriori estimation

% prem: removed reference to stochastic inference
%       we should add this in future work/discussion
%% Furthermore, using variational inference opens the door to scaling to
%% massive data sets, even larger than the data sets we analyze in this
%% paper, using stochastic variational inference~\cite{Hoffman:2013} (see
%% \mysec{inference}).

Independently of GaP, Bayesian Poisson factorization has been studied
in the signal processing community for performing source separation
from spectrogram data~\cite{Cemgil:2009,Hoffman:2012}.  This research
includes variational approximations to the posterior, though the
issues and details around spectrogram data differ significantly from
user behavior data we consider and our derivation below (based on
auxiliary variables) is more direct.  As future work, the methods
developed here could lead to improved methods for massive simultaneous
analysis of audio spectrograms. 

In the context of network data, a Poisson model of overlapping
communities was described by the authors of ~\cite{Ball:2011}, and has
been shown to work well in recovering overlapping communities in
network data~\cite{Gopalan:2013}.  As with GaP, this model is
unregularized and the authors fit the model with maximum likelihood
via expectation maximization.

There has been significant research on Bayesian nonparametric Poisson
factor models. Using a Beta-Negative-Binomial process prior for
Poisson factor analysis the authors of ~\cite{Zhou:2012} demonstrate
that NMF, LDA and GaP are special cases of a finite approximation of
their model. Both BPF and HPF does not immediately fall out of their
framework, and placing Bayesian nonparametric priors on our models is
an ongoing research effort.

Probabilistic Matrix Factorization~\cite{Salakhutdinov:2008a} (PMF)
places spherical Gaussian priors on the latent user preferences and
latent item attributes, and is aimed at factorizing explicit feedback.
PMF uses gradient descent for inference, while the fully Bayesian
treatment uses an Monte Carlo Markov Chain (MCMC) algorithm
~\cite{Salakhutdinov:2008}.  In \mysec{eval}, we show significantly
improved performance compared to gradient descent algorithm while the
MCMC algorithm scales poorly to our datasets.

With implicit feedback, researchers have proposed treating the data as
indication of positive and negative preference associated with varying
confidence levels~\cite{Hu:2008p9402}.


We discuss further differences between our method and previous work on
matrix factorization in the empirical results below.



% !!! add reference to canny's click data
% !!! add reference to SIGIR
XXX \cite{Marlin:2009,Marlin:2012,Elkan:2008,Ma:2011}
