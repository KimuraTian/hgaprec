\section{Related work}
The roots of Poisson factorization come from nonnegative matrix
factorization~\cite{Lee:1999}, where the objective function is
equivalent to a factorized Poisson likelihood.  The original NMF
update equations have been shown to be an expectation-maximisation
(EM) algorithm for maximum likelihood estimation of a Poisson model
via data augmentation~\cite{Cemgil:2009}.

Placing a Gamma prior on the user weights gives the GaP
model~\cite{Canny:2004}, which was developed as an alternative text
model to latent Dirichlet allocation (LDA)~\cite{Blei:2003b}. Placing
a Gamma prior on both the user and item weights gives the
Probabilistic Factor Model (PFM)~\cite{Ma:2011}.

A difference between our treatment and GaP is that GaP fits the item
weights with maximum likelihood via the expectation maximization
algorithm. We approximate the full posterior with variational
inference. Further, our model places a hierarchical prior structure
using Gamma priors on weights, and Gamma priors on user and item rate
parameters.  Placing priors on both sets of weights further
regularizes the model and lets us use the same inferential machinery
in both user-space and item-space. 

Although the PFM~\cite{Ma:2011} is the same as the BPF, the PFM was
aimed at a particular application---that of recommending
web-sites---while we demonstrate improved performance on a variety of
recommendation tasks and study why Poisson factorization based models
outperform others. We also extended the BPF by placing priors on rate
parameters to capture the skew in user activity and item
popularity. Capturing these biases is important for predictive
performance~\cite{Koren:2009}. Again, we approximate the full
posterior with variational inference, while PFM uses MAP inference.

% prem: removed reference to stochastic inference
%       we should add this in future work/discussion
%% Furthermore, using variational inference opens the door to scaling to
%% massive data sets, even larger than the data sets we analyze in this
%% paper, using stochastic variational inference~\cite{Hoffman:2013} (see
%% \mysec{inference}).

We note that GaP was developed as an alternative to LDA. In Appendix
A, we show that LDA can be reinterpreted as an instance of Poisson
factorization where we condition on the user counts and use an
alternative prior on the item weights.  (This connection was
previously unknown.)

Independently of GaP, Bayesian Poisson factorization has been studied
in the signal processing community for performing source separation
from spectrogram data~\cite{Cemgil:2009,Hoffman:2012}.  This research
includes variational approximations to the posterior, though the
issues and details around spectrogram data differ significantly from
user behavior data we consider and our derivation below (based on
auxiliary variables) is more direct.  As future work, the methods
developed here could lead to improved methods for massive simultaneous
analysis of audio spectrograms. In the context of network data, a
Poisson model of overlapping communities was described by the authors
of ~\cite{Ball:2011}. As with GaP, this model is unregularized and the
authors fit the model with maximum likelihood via expectation
maximization.

We discuss further differences between our method and previous work on
matrix factorization in the empirical results below.

% !!! add reference to canny's click data
% !!! add reference to SIGIR
XXX \cite{Marlin:2009,Marlin:2012,Elkan:2008,Ma:2011}
